{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Project for Natural Language Understanding Course\n",
        "\n",
        "Andrea Bonora \n",
        "mat. 232222"
      ],
      "metadata": {
        "id": "RZ2DJAKd0la5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "aE8NpQ4_aeD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Paths\n",
        "#@markdown\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/NLU_project_Andrea_Bonora_232222/ptbdataset.zip'  #@param {type: \"string\"}\n",
        "models_path = '/content/drive/MyDrive/NLU_project_Andrea_Bonora_232222/NLU_models'  #@param {type: \"string\"}\n",
        "#@markdown ---\n"
      ],
      "metadata": {
        "id": "e3iBt8EdZHrn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_Dt1a9neYRW",
        "outputId": "c7c764a4-5b38-4daf-b737-92dd8337a3b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZFhGjztZehfd"
      },
      "outputs": [],
      "source": [
        "!cp -r $dataset_path ./ptbdataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SckrbB7YCjBi",
        "outputId": "7970b6ef-a1e7-4282-a7f6-b0304a5a77f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ptbdataset.zip\n",
            "  inflating: ptbdataset/ptb.char.test.txt  \n",
            "  inflating: ptbdataset/ptb.char.train.txt  \n",
            "  inflating: ptbdataset/ptb.char.valid.txt  \n",
            "  inflating: ptbdataset/ptb.test.txt  \n",
            "  inflating: ptbdataset/ptb.train.txt  \n",
            "  inflating: ptbdataset/ptb.valid.txt  \n",
            "  inflating: ptbdataset/README       \n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/ptbdataset.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Parameters"
      ],
      "metadata": {
        "id": "AA7R31_QamHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6hO8N2Xb9SKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a204c383-f8ec-4c04-ac05-04e8035c9a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-f9531d4711ed>:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
            "  from collections import Iterable\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Iterable\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parameters\n",
        "#@markdown Insert value for the following parameters\n",
        "\n",
        "emsize = 1500  #@param {type: \"number\"}\n",
        "nhid = 1500  #@param {type: \"number\"}\n",
        "nlayers = 2  #@param {type: \"slider\", min: 1, max: 5}\n",
        "bptt = 35  #@param {type: \"number\"}\n",
        "dropout = 0.65 #@param {type: \"number\"}\n",
        "clip = 0.25 #@param {type: \"number\"}\n",
        "tied = True #@param{type: \"boolean\"}\n",
        "alpha = 2 #@param {type: \"number\"}\n",
        "beta = 1 #@param {type: \"number\"}\n",
        "#@markdown ---\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "torch.manual_seed(1111)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxn1cOOVaquH",
        "outputId": "b0ef34b0-6fb6-49e7-df15-176a6fe5ea81"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3c1c96c390>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model, Corpus, Dictionary and Positional encoding"
      ],
      "metadata": {
        "id": "8FjKwVhQcjAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, dropout=0.5):\n",
        "        if not self.training or not dropout:\n",
        "            return x\n",
        "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
        "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x"
      ],
      "metadata": {
        "id": "57h5MCv8wsJW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DTS-ZRdF_4mo"
      },
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.lockdrop = LockedDropout()\n",
        "        self.dropout = dropout\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.rnn = getattr(nn, \"LSTM\")(ninp, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden, return_h = False):\n",
        "        emb = self.lockdrop(self.encoder(input), self.dropout)\n",
        "        raw_output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.lockdrop(raw_output, dropout)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        if return_h:\n",
        "          return F.log_softmax(decoded, dim=1), hidden, raw_output, output\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3M_4ncUn_mVt"
      },
      "outputs": [],
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Kk6076Ux_qL7"
      },
      "outputs": [],
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'ptb.train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'ptb.valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'ptb.test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "Grj6fSj4cwCV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7i0ZHIdqDiC3"
      },
      "outputs": [],
      "source": [
        "def batchify(data: Iterable, bsz:int):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IX3LXP40EXFf"
      },
      "outputs": [],
      "source": [
        "def repackage_hidden(h: torch.Tensor):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9OCgwE2QEawC"
      },
      "outputs": [],
      "source": [
        "def get_batch(source: Iterable, i: int):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KhaJXSAzbn4b"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(type: str, model: nn.Module, lr: float):\n",
        "  if type == \"SGD\":\n",
        "    return torch.optim.SGD(model.parameters(), lr=lr, weight_decay=1.2e-6)\n",
        "  elif type == \"ASGD\":\n",
        "    return torch.optim.ASGD(model.parameters(), lr=lr, weight_decay = 1.2e-6)\n",
        "  elif type == \"Adam\":\n",
        "    return torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.99))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing"
      ],
      "metadata": {
        "id": "1MBS27K4c20p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "O989YJhAEd-m"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, data_source: Iterable):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "\n",
        "    return total_loss / (len(data_source) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Uc4dV6usEgyY"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module,\n",
        "          opt: any,\n",
        "          train_data: Iterable,\n",
        "          epoch: int):\n",
        "  \n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    pbar = tqdm(range(0, train_data.size(0) - 1, bptt),position=0, leave=False)\n",
        "    for p, (batch, i) in zip(pbar, enumerate(range(0, train_data.size(0) - 1, bptt))):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        opt.zero_grad()\n",
        "        \n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        output, hidden, rnn_hs, dropped_rnn_hs = model(data, hidden, return_h = True)\n",
        "\n",
        "        loss = criterion(output, targets)\n",
        "        \n",
        "        # Activiation Regularization\n",
        "        if alpha: loss = loss + alpha * dropped_rnn_hs[-1:].pow(2).mean() \n",
        "        # Temporal Activation Regularization (slowness)\n",
        "        if beta: loss = loss + beta * (rnn_hs[-1][1:] - rnn_hs[-1][:-1]).pow(2).mean()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "        for p in model.parameters():\n",
        "          p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print('| epoch {:3d} | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, opt.param_groups[0]['lr'], total_loss/batch, math.exp(total_loss/batch)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "a37o_g54XE7o"
      },
      "outputs": [],
      "source": [
        "def test(model, test_data):\n",
        "  test_loss = evaluate(model, test_data)\n",
        "  print('=' * 89)\n",
        "  print('| Test | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "  print('=' * 89)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run experiments"
      ],
      "metadata": {
        "id": "0SdBfahaeQ8g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dS5V45vnTf-d"
      },
      "outputs": [],
      "source": [
        "def main(lr:float,\n",
        "         batch_size:int,\n",
        "         eval_batch_size:int,\n",
        "         epochs:int,\n",
        "         pretrained:bool = True,\n",
        "         training:bool = True,\n",
        "         n:int = 3):\n",
        "  \n",
        "  '''\n",
        "   Args:\n",
        "      lr: learning rate\n",
        "      batch_size: batch size for the training data\n",
        "      eval_batch_size: batch size for the evaluation and testing data\n",
        "      epochs: number of epochs to train the model\n",
        "      pretrained: true to use an already trained model in the models folder\n",
        "      training: true to train the model, false to test only. Use training = True with pretrained = True. \n",
        "      n: number of successive iterations without improvement to trigger the ASGD optimizer and learning rate annealing\n",
        "  '''\n",
        "  \n",
        "  corpus = Corpus('/content/ptbdataset')\n",
        "  train_data = batchify(corpus.train, batch_size)\n",
        "  val_data = batchify(corpus.valid, eval_batch_size)\n",
        "  ntokens = len(corpus.dictionary)\n",
        "  \n",
        "  model = RNNModel(ntokens, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "  opt = get_optimizer(\"SGD\", model, lr) \n",
        "  no_imp = n\n",
        "  best_val_loss = None\n",
        "\n",
        "  if training:\n",
        "    if pretrained:\n",
        "      model.load_state_dict(torch.load(models_path + '/best_model.pt'))\n",
        "      best_val_loss = evaluate(model, val_data)\n",
        "      print(\"Best loss reached:\", best_val_loss)\n",
        "      model.load_state_dict(torch.load(models_path + '/last_model.pt'))\n",
        "      opt = get_optimizer(\"SGD\", model, lr)\n",
        "    \n",
        "    pbar = tqdm(range(1, epochs+1),position=0, leave=False)\n",
        "    for p, epoch in zip(pbar,range(1, epochs+1)):\n",
        "        epoch_start_time = time.time()\n",
        "        train(model, opt, train_data, epoch)\n",
        "        val_loss = evaluate(model, val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            torch.save(model.state_dict(), models_path + '/best_model.pt')\n",
        "            best_val_loss = val_loss\n",
        "            no_imp = n\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            no_imp -= 1\n",
        "            if no_imp == 0:\n",
        "              no_imp = n\n",
        "              lr /= 4\n",
        "              opt.param_groups[0]['lr'] = lr\n",
        "        \n",
        "        torch.save(model.state_dict(), models_path + '/last_model.pt')\n",
        "\n",
        "  model.load_state_dict(torch.load(models_path + '/best_model.pt'))\n",
        "  model.rnn.flatten_parameters()\n",
        "  test(model, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Running parameters\n",
        "#@markdown Running parameters\n",
        "\n",
        "lr = 20  #@param {type: \"number\"}\n",
        "epochs = 40 #@param {type: \"number\"}\n",
        "batch_size = 20 #@param {type: \"number\"}\n",
        "eval_batch_size = 10 #@param {type: \"number\"}\n",
        "training = True #@param {type: \"boolean\"}\n",
        "pretrained = False #@param {type: \"boolean\"}\n",
        "n = 3 #@param {type: \"number\"}\n",
        "#@markdown ---\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "P5Ytq-e4fXhM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtadgyBtvkvg"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  main(lr, batch_size, eval_batch_size, epochs, pretrained = pretrained, training = training, n = n)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = Corpus('/content/ptbdataset')\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = RNNModel(ntokens, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "print(\"Model1\")\n",
        "model.load_state_dict(torch.load(models_path+'/model1.pt'))\n",
        "model.rnn.flatten_parameters()\n",
        "test(model, test_data)\n",
        "\n",
        "print(\"Model2\")\n",
        "model.load_state_dict(torch.load(models_path+'/model2.pt'))\n",
        "model.rnn.flatten_parameters()\n",
        "test(model, test_data)\n",
        "\n",
        "print(\"Model3\")\n",
        "model.load_state_dict(torch.load(models_path+'/model3.pt'))\n",
        "model.rnn.flatten_parameters()\n",
        "test(model, test_data)\n",
        "\n",
        "print(\"Model4\")\n",
        "model.load_state_dict(torch.load(models_path+'/model4.pt'))\n",
        "model.rnn.flatten_parameters()\n",
        "test(model, test_data)\n",
        "\n",
        "print(\"Model5\")\n",
        "model.load_state_dict(torch.load(models_path+'/model5.pt'))\n",
        "model.rnn.flatten_parameters()\n",
        "test(model, test_data)\n",
        "\n",
        "print(\"Model6\")\n",
        "model.load_state_dict(torch.load(models_path+'/model6.pt'))\n",
        "model.rnn.flatten_parameters()\n",
        "test(model, test_data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqpMR57dcZyI",
        "outputId": "0344ccd3-3fe3-4213-a252-585893d45364"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model1\n",
            "=========================================================================================\n",
            "| Test | test loss  4.35 | test ppl    77.33\n",
            "=========================================================================================\n",
            "Model2\n",
            "=========================================================================================\n",
            "| Test | test loss  4.35 | test ppl    77.47\n",
            "=========================================================================================\n",
            "Model3\n",
            "=========================================================================================\n",
            "| Test | test loss  4.34 | test ppl    76.68\n",
            "=========================================================================================\n",
            "Model4\n",
            "=========================================================================================\n",
            "| Test | test loss  4.32 | test ppl    75.47\n",
            "=========================================================================================\n",
            "Model5\n",
            "=========================================================================================\n",
            "| Test | test loss  4.31 | test ppl    74.52\n",
            "=========================================================================================\n",
            "Model6\n",
            "=========================================================================================\n",
            "| Test | test loss  4.34 | test ppl    77.04\n",
            "=========================================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}